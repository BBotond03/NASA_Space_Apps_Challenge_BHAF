{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLq6QKnK8K5jry5uNes37D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BBotond03/NASA_Space_Apps_Challenge_BHAF/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fullos mullos accuracy néző\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#import pywt  # Importing the PyWavelets library for wavelet transform\n",
        "\n",
        "# Try reading it as a CSV\n",
        "\n",
        "#file_path = r\"C:\\Users\\aurel\\Downloads\\TrainingData-20241006T092404Z-001\\TrainingData\\fractioned_data_4859152.csv\"\n",
        "df = pd.read_csv('seismic_data2.csv', skiprows=1, header=None, names=[\"Velocity\", \"Quake_type\"])\n",
        "\n",
        "# Drop the AbsoluteTime column, keep only relative time and velocity\n",
        "df = df[[\"Velocity\", \"Quake_type\"]]\n",
        "\n",
        "data = df.to_numpy()\n",
        "\n",
        "# Extract relative times and velocities\n",
        "velocities = data[:, 0]  # Relative time in seconds (starts from 0.0, increases with time)\n",
        "quake_type = data[:, 1]  # Quake velocity\n",
        "\n",
        "events = []\n",
        "\n",
        "for index, event in enumerate(quake_type):\n",
        "  if event == 1:\n",
        "    events.append(index)\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# Load your data into a DataFrame (replace the file path with your actual data file)\n",
        "# df = pd.read_csv('your_data.csv')  # Uncomment this line and provide the correct path to your data file\n",
        "\n",
        "# Sample data creation for illustration purposes (replace with your actual dataframe)\n",
        "# df = pd.DataFrame({'RelativeTime': np.linspace(0, 100000, 100000), 'wavelet_filtered_output': np.sin(np.linspace(0, 100000, 100000) / 10000) * 1e-25})\n",
        "\n",
        "# Extract the columns\n",
        "#relative_time = df['RelativeTime']\n",
        "quake_velocity = velocities\n",
        "#relative_time = relative_time[0:400000]\n",
        "#quake_velocity = velocities[0:400000]\n",
        "# Calculate threshold dynamically based on data characteristics\n",
        "mean_value = np.mean(quake_velocity)\n",
        "std_deviation = np.std(quake_velocity)\n",
        "peak_height = mean_value + 3.8 * std_deviation  # Setting threshold as 3 standard deviations above the mean\n",
        "\n",
        "# Identify peaks\n",
        "peaks, properties = find_peaks(np.abs(quake_velocity), height=peak_height)\n",
        "\n",
        "#print(\"peak data:\")\n",
        "#print(peaks)\n",
        "#print(properties)\n",
        "\n",
        "# Filter out isolated peaks\n",
        "significant_clusters = []\n",
        "current_cluster = []\n",
        "\n",
        "for peak in peaks:\n",
        "    if not current_cluster or peak - current_cluster[-1] <= 10:  # Adjust this as necessary to control cluster width\n",
        "        current_cluster.append(peak)\n",
        "    else:\n",
        "        if len(current_cluster) > 10:\n",
        "            significant_clusters.append(current_cluster)\n",
        "        current_cluster = [peak]\n",
        "\n",
        "# Add the last cluster if it has more than one peak\n",
        "if len(current_cluster) > 9600:\n",
        "    significant_clusters.append(current_cluster)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "events = []\n",
        "\n",
        "for index, event in enumerate(quake_type):\n",
        "  if event == 1:\n",
        "    events.append(index)\n",
        "\n",
        "\n",
        "detected_events = 0\n",
        "for i in significant_clusters:\n",
        "  for j in i:\n",
        "    detected_events +=1\n",
        "\n",
        "\n",
        "good_detection = 0\n",
        "confidence_interval = 6800\n",
        "\n",
        "for i in significant_clusters:\n",
        "  for j in i:\n",
        "    not_detected = True\n",
        "    for b in events:\n",
        "      if not_detected:\n",
        "        if b-confidence_interval <j < b+confidence_interval:\n",
        "            good_detection +=1\n",
        "            not_detected = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"This tells us how many of the algorithms detections were correct\")\n",
        "print(good_detection/detected_events)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVmiBl2MIZ5V",
        "outputId": "35479f63-3760-45f3-c7dd-23cae4d92532"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This tells us how many of the algorithms detections were correct\n",
            "0.9938650306748467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWWneVM8sgI3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}